{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uGLQ0V0XxeE"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from scipy.optimize import minimize\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9e9EcJyaVVI"
      },
      "source": [
        "def preprocess():\n",
        "    \"\"\" \n",
        "     Input:\n",
        "     Although this function doesn't have any input, you are required to load\n",
        "     the MNIST data set from file 'mnist_all.mat'.\n",
        "\n",
        "     Output:\n",
        "     train_data: matrix of training set. Each row of train_data contains \n",
        "       feature vector of a image\n",
        "     train_label: vector of label corresponding to each image in the training\n",
        "       set\n",
        "     validation_data: matrix of training set. Each row of validation_data \n",
        "       contains feature vector of a image\n",
        "     validation_label: vector of label corresponding to each image in the \n",
        "       training set\n",
        "     test_data: matrix of training set. Each row of test_data contains \n",
        "       feature vector of a image\n",
        "     test_label: vector of label corresponding to each image in the testing\n",
        "       set\n",
        "    \"\"\"\n",
        "\n",
        "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
        "\n",
        "    n_feature = mat.get(\"train1\").shape[1]\n",
        "    n_sample = 0\n",
        "    for i in range(10):\n",
        "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
        "    n_validation = 1000\n",
        "    n_train = n_sample - 10 * n_validation\n",
        "\n",
        "    # Construct validation data\n",
        "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
        "    for i in range(10):\n",
        "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
        "\n",
        "    # Construct validation label\n",
        "    validation_label = np.ones((10 * n_validation, 1))\n",
        "    for i in range(10):\n",
        "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
        "\n",
        "    # Construct training data and label\n",
        "    train_data = np.zeros((n_train, n_feature))\n",
        "    train_label = np.zeros((n_train, 1))\n",
        "    temp = 0\n",
        "    for i in range(10):\n",
        "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
        "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
        "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
        "        temp = temp + size_i - n_validation\n",
        "\n",
        "    # Construct test data and label\n",
        "    n_test = 0\n",
        "    for i in range(10):\n",
        "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
        "    test_data = np.zeros((n_test, n_feature))\n",
        "    test_label = np.zeros((n_test, 1))\n",
        "    temp = 0\n",
        "    for i in range(10):\n",
        "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
        "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
        "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
        "        temp = temp + size_i\n",
        "\n",
        "    # Delete features which don't provide any useful information for classifiers\n",
        "    sigma = np.std(train_data, axis=0)\n",
        "    index = np.array([])\n",
        "    for i in range(n_feature):\n",
        "        if (sigma[i] > 0.001):\n",
        "            index = np.append(index, [i])\n",
        "    train_data = train_data[:, index.astype(int)]\n",
        "    validation_data = validation_data[:, index.astype(int)]\n",
        "    test_data = test_data[:, index.astype(int)]\n",
        "\n",
        "    # Scale data to 0 and 1\n",
        "    train_data /= 255.0\n",
        "    validation_data /= 255.0\n",
        "    test_data /= 255.0\n",
        "\n",
        "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdVJLd0CahGB"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCIfdTWdamxM"
      },
      "source": [
        "def blrObjFunction(initialWeights, *args):\n",
        "    \"\"\"\n",
        "    blrObjFunction computes 2-class Logistic Regression error function and\n",
        "    its gradient.\n",
        "\n",
        "    Input:\n",
        "        initialWeights: the weight vector (w_k) of size (D + 1) x 1 \n",
        "        train_data: the data matrix of size N x D\n",
        "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
        "\n",
        "    Output: \n",
        "        error: the scalar value of error function of 2-class logistic regression\n",
        "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
        "                    error function\n",
        "    \"\"\"\n",
        "    train_data, labeli = args\n",
        "\n",
        "    n_data = train_data.shape[0]\n",
        "    n_features = train_data.shape[1]\n",
        "    error = 0\n",
        "    error_grad = np.zeros((n_features + 1, 1))\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "\n",
        "    X = np.concatenate((np.ones((n_data, 1)), train_data), axis = 1)\n",
        "    # print(X.shape)\n",
        "    # print(initialWeights.shape)\n",
        "    w = np.array(initialWeights.reshape((n_features + 1, 1)))\n",
        "    error = (-1.0 / n_data) * (np.sum(labeli * np.log(sigmoid(X.dot(w))) + (1.0 - labeli) * np.log(1.0 - sigmoid(X.dot(w)))))\n",
        "    error_grad = (1.0 / n_data) * np.sum((sigmoid(X.dot(w)) - labeli) * X, axis = 0)\n",
        "\n",
        "    return error, error_grad"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9rEtqg-asVC"
      },
      "source": [
        "def blrPredict(W, data):\n",
        "    \"\"\"\n",
        "     blrObjFunction predicts the label of data given the data and parameter W \n",
        "     of Logistic Regression\n",
        "     \n",
        "     Input:\n",
        "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight \n",
        "         vector of a Logistic Regression classifier.\n",
        "         X: the data matrix of size N x D\n",
        "         \n",
        "     Output: \n",
        "         label: vector of size N x 1 representing the predicted label of \n",
        "         corresponding feature vector given in data matrix\n",
        "\n",
        "    \"\"\"\n",
        "    label = np.zeros((data.shape[0], 1))\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "\n",
        "    X = np.concatenate((np.ones((data.shape[0], 1)), data), axis = 1)\n",
        "    # print(X.shape)\n",
        "    # print(W.shape)\n",
        "    label = np.argmax(sigmoid(X.dot(W)), axis = 1)\n",
        "    label = label.reshape((data.shape[0], 1))\n",
        "\n",
        "    return label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkFg-3qIa2Jk"
      },
      "source": [
        "def mlrObjFunction(params, *args):\n",
        "    \"\"\"\n",
        "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
        "    its gradient.\n",
        "\n",
        "    Input:\n",
        "        initialWeights_b: the weight vector of size (D + 1) x 10\n",
        "        train_data: the data matrix of size N x D\n",
        "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
        "                representing the label of corresponding feature vector\n",
        "\n",
        "    Output:\n",
        "        error: the scalar value of error function of multi-class logistic regression\n",
        "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
        "                    error function\n",
        "    \"\"\"\n",
        "    train_data, labeli = args\n",
        "    n_data = train_data.shape[0]\n",
        "    n_feature = train_data.shape[1]\n",
        "    error = 0\n",
        "    error_grad = np.zeros((n_feature + 1, n_class))\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "\n",
        "    X = np.concatenate((np.ones((n_data, 1)), train_data), axis = 1)\n",
        "    # print(X.shape)\n",
        "    # print(params.shape)\n",
        "    w_b = np.array(params.reshape((n_feature+1, n_class)))\n",
        "    theta = np.exp(X.dot(w_b)) / (np.sum(np.exp(X.dot(w_b)), axis = 1).reshape((train_data.shape[0], 1)))\n",
        "    # print(theta.shape)\n",
        "    # print(labeli.shape)\n",
        "    # print((labeli * np.log(theta)).shape)\n",
        "    # error = -1.0 * np.sum(labeli * np.log(theta))\n",
        "    error = -1.0 * np.sum(np.sum((labeli * np.log(theta)), axis = 1), axis = 0)\n",
        "    error_grad = np.transpose(X).dot(theta - labeli)\n",
        "    # print(error_grad.shape)\n",
        "    error_grad = error_grad.flatten()\n",
        "\n",
        "    return error, error_grad"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fuosbCBa8vc"
      },
      "source": [
        "def mlrPredict(W, data):\n",
        "    \"\"\"\n",
        "     mlrObjFunction predicts the label of data given the data and parameter W\n",
        "     of Logistic Regression\n",
        "\n",
        "     Input:\n",
        "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
        "         vector of a Logistic Regression classifier.\n",
        "         X: the data matrix of size N x D\n",
        "\n",
        "     Output:\n",
        "         label: vector of size N x 1 representing the predicted label of\n",
        "         corresponding feature vector given in data matrix\n",
        "\n",
        "    \"\"\"\n",
        "    label = np.zeros((data.shape[0], 1))\n",
        "\n",
        "    ##################\n",
        "    # YOUR CODE HERE #\n",
        "    ##################\n",
        "    # HINT: Do not forget to add the bias term to your input data\n",
        "\n",
        "    X = np.concatenate((np.ones((data.shape[0], 1)), data), axis = 1)\n",
        "    # print(X.shape)\n",
        "    # print(W.shape)\n",
        "    theta = np.exp(X.dot(W)) / (np.sum(np.exp(X.dot(W)), axis = 1).reshape((data.shape[0], 1)))\n",
        "    label = np.argmax(theta, axis = 1)\n",
        "    # print(label.shape)\n",
        "    label = label.reshape((data.shape[0], 1))\n",
        "\n",
        "    return label"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KhPw5JEbEON",
        "outputId": "db20ab86-2c46-4105-fd6a-f022b1d658b7"
      },
      "source": [
        "\"\"\"\n",
        "Script for Logistic Regression for Binary Classification\n",
        "\"\"\"\n",
        "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
        "\n",
        "# number of classes\n",
        "n_class = 10\n",
        "\n",
        "# number of training samples\n",
        "n_train = train_data.shape[0]\n",
        "\n",
        "# number of features\n",
        "n_feature = train_data.shape[1]\n",
        "\n",
        "Y = np.zeros((n_train, n_class))\n",
        "for i in range(n_class):\n",
        "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
        "\n",
        "# Logistic Regression with Gradient Descent\n",
        "W = np.zeros((n_feature + 1, n_class))\n",
        "initialWeights = np.zeros((n_feature + 1, 1))\n",
        "opts = {'maxiter': 100}\n",
        "for i in tqdm(range(n_class)):\n",
        "    labeli = Y[:, i].reshape(n_train, 1)\n",
        "    args = (train_data, labeli)\n",
        "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
        "    W[:, i] = nn_params.x.reshape((n_feature + 1,))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [11:23<00:00, 68.33s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ate7A-LbVfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf5c8c1-bedd-444d-957a-3741bb231086"
      },
      "source": [
        "print('\\n------------------- Training Data Set (Logistic Regression for Binary Classification) -------------------')    \n",
        "\n",
        "train_data_order = np.array([[train_data[j] for j in range(len(train_label)) if train_label[j] == i] for i in range(10)])\n",
        "#   separate each category\n",
        "\n",
        "for i in range(10):\n",
        "    train_data_order[i] = np.array(train_data_order[i])    \n",
        "    #   change the <class 'list'> to <class 'numpy.ndarray'>\n",
        "\n",
        "for i in range(10):\n",
        "    predicted_label = blrPredict(W, train_data_order[i])\n",
        "    print('\\n Training set of category ' + str(i) + ' Accuracy: ' + str(100 * np.mean((predicted_label == i).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Training Dataset\n",
        "predicted_label = blrPredict(W, train_data)\n",
        "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------- Training Data Set (Logistic Regression for Binary Classification) -------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training set of category 0 Accuracy: 97.84684135689619%\n",
            "\n",
            " Training set of category 1 Accuracy: 97.91013584117032%\n",
            "\n",
            " Training set of category 2 Accuracy: 91.10528438886648%\n",
            "\n",
            " Training set of category 3 Accuracy: 89.80705515494056%\n",
            "\n",
            " Training set of category 4 Accuracy: 93.90747624948368%\n",
            "\n",
            " Training set of category 5 Accuracy: 88.30581316444244%\n",
            "\n",
            " Training set of category 6 Accuracy: 96.29930866205775%\n",
            "\n",
            " Training set of category 7 Accuracy: 94.33998100664768%\n",
            "\n",
            " Training set of category 8 Accuracy: 87.54895897753042%\n",
            "\n",
            " Training set of category 9 Accuracy: 88.7654071529602%\n",
            "\n",
            " Training set Accuracy:92.716%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGID-7UUbn29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccb0f64-8d9c-4cdf-8f7f-b94dff766aab"
      },
      "source": [
        "print('\\n------------------- Validation Data Set (Logistic Regression for Binary Classification) -------------------')\n",
        "\n",
        "validation_data_order = np.array([[validation_data[j] for j in range(len(validation_label)) if validation_label[j] == i] for i in range(10)])\n",
        "#   separate each category\n",
        "\n",
        "for i in range(10):\n",
        "    validation_data_order[i] = np.array(validation_data_order[i])    \n",
        "    #   change the <class 'list'> to <class 'numpy.ndarray'>\n",
        "\n",
        "for i in range(10):\n",
        "    predicted_label = blrPredict(W, validation_data_order[i])\n",
        "    print('\\n Validation set of category ' + str(i) + ' Accuracy: ' + str(100 * np.mean((predicted_label == i).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Validation Dataset\n",
        "predicted_label = blrPredict(W, validation_data)\n",
        "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------- Validation Data Set (Logistic Regression for Binary Classification) -------------------\n",
            "\n",
            " Validation set of category 0 Accuracy: 97.6%\n",
            "\n",
            " Validation set of category 1 Accuracy: 96.8%\n",
            "\n",
            " Validation set of category 2 Accuracy: 88.0%\n",
            "\n",
            " Validation set of category 3 Accuracy: 88.6%\n",
            "\n",
            " Validation set of category 4 Accuracy: 93.7%\n",
            "\n",
            " Validation set of category 5 Accuracy: 86.8%\n",
            "\n",
            " Validation set of category 6 Accuracy: 95.7%\n",
            "\n",
            " Validation set of category 7 Accuracy: 92.60000000000001%\n",
            "\n",
            " Validation set of category 8 Accuracy: 84.6%\n",
            "\n",
            " Validation set of category 9 Accuracy: 90.0%\n",
            "\n",
            " Validation set Accuracy:91.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mv-s6GZb6V5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cad84bc-6995-4bc6-ab13-355298609e21"
      },
      "source": [
        "print('\\n------------------- Testing Data Set (Logistic Regression for Binary Classification) -------------------')\n",
        "\n",
        "test_data_order = np.array([[test_data[j] for j in range(len(test_label)) if test_label[j] == i] for i in range(10)])\n",
        "#   separate each category\n",
        "\n",
        "for i in range(10):\n",
        "    test_data_order[i] = np.array(test_data_order[i])    \n",
        "    #   change the <class 'list'> to <class 'numpy.ndarray'>\n",
        "\n",
        "for i in range(10):\n",
        "    predicted_label = blrPredict(W, test_data_order[i])\n",
        "    print('\\n Testing set of category ' + str(i) + ' Accuracy: ' + str(100 * np.mean((predicted_label == i).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Testing Dataset\n",
        "predicted_label = blrPredict(W, test_data)\n",
        "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------- Testing Data Set (Logistic Regression for Binary Classification) -------------------\n",
            "\n",
            " Testing set of category 0 Accuracy: 98.06122448979592%\n",
            "\n",
            " Testing set of category 1 Accuracy: 98.14977973568281%\n",
            "\n",
            " Testing set of category 2 Accuracy: 89.05038759689923%\n",
            "\n",
            " Testing set of category 3 Accuracy: 91.0891089108911%\n",
            "\n",
            " Testing set of category 4 Accuracy: 93.38085539714868%\n",
            "\n",
            " Testing set of category 5 Accuracy: 85.65022421524664%\n",
            "\n",
            " Testing set of category 6 Accuracy: 94.88517745302714%\n",
            "\n",
            " Testing set of category 7 Accuracy: 92.60700389105058%\n",
            "\n",
            " Testing set of category 8 Accuracy: 87.06365503080082%\n",
            "\n",
            " Testing set of category 9 Accuracy: 89.09811694747275%\n",
            "\n",
            " Testing set Accuracy:92.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QKrs-sIcA94"
      },
      "source": [
        "\"\"\"\n",
        "Script for Logistic Regression for Multiple Classification\n",
        "\"\"\"\n",
        "# FOR EXTRA CREDIT ONLY\n",
        "W_b = np.zeros((n_feature + 1, n_class))\n",
        "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
        "opts_b = {'maxiter': 100}\n",
        "\n",
        "args_b = (train_data, Y)\n",
        "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
        "W_b = nn_params.x.reshape((n_feature + 1, n_class))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUrqadqGcdY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cdcaa9-ceb9-4288-d6ab-8299490fedc6"
      },
      "source": [
        "print('\\n------------------- Training Data Set (Logistic Regression for Multiple Classification) -------------------')    \n",
        "\n",
        "train_data_order_b = np.array([[train_data[j] for j in range(len(train_label)) if train_label[j] == i] for i in range(10)])\n",
        "#   separate each category\n",
        "\n",
        "for i in range(10):\n",
        "    train_data_order_b[i] = np.array(train_data_order_b[i])    \n",
        "    #   change the <class 'list'> to <class 'numpy.ndarray'>\n",
        "\n",
        "for i in range(10):\n",
        "    predicted_label_b = mlrPredict(W_b, train_data_order_b[i])\n",
        "    print('\\n Training set of category ' + str(i) + ' Accuracy: ' + str(100 * np.mean((predicted_label_b == i).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Training Dataset\n",
        "predicted_label_b = mlrPredict(W_b, train_data)\n",
        "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------- Training Data Set (Logistic Regression for Multiple Classification) -------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training set of category 0 Accuracy: 97.17651838309975%\n",
            "\n",
            " Training set of category 1 Accuracy: 97.47474747474747%\n",
            "\n",
            " Training set of category 2 Accuracy: 90.8229124647035%\n",
            "\n",
            " Training set of category 3 Accuracy: 90.6061196647827%\n",
            "\n",
            " Training set of category 4 Accuracy: 93.82486575795126%\n",
            "\n",
            " Training set of category 5 Accuracy: 89.23320515720425%\n",
            "\n",
            " Training set of category 6 Accuracy: 96.1569743798292%\n",
            "\n",
            " Training set of category 7 Accuracy: 94.22602089268756%\n",
            "\n",
            " Training set of category 8 Accuracy: 89.3630179344465%\n",
            "\n",
            " Training set of category 9 Accuracy: 91.23055162659122%\n",
            "\n",
            " Training set Accuracy:93.122%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-NE861sckBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be8943b-5149-423d-c825-f9ae4e164fd8"
      },
      "source": [
        "print('\\n------------------- Validation Data Set (Logistic Regression for Multiple Classification) -------------------')\n",
        "\n",
        "validation_data_order_b = np.array([[validation_data[j] for j in range(len(validation_label)) if validation_label[j] == i] for i in range(10)])\n",
        "#   separate each category\n",
        "\n",
        "for i in range(10):\n",
        "    validation_data_order_b[i] = np.array(validation_data_order_b[i])    \n",
        "    #   change the <class 'list'> to <class 'numpy.ndarray'>\n",
        "\n",
        "for i in range(10):\n",
        "    predicted_label_b = mlrPredict(W_b, validation_data_order_b[i])\n",
        "    print('\\n Validation set of category ' + str(i) + ' Accuracy: ' + str(100 * np.mean((predicted_label_b == i).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Validation Dataset\n",
        "predicted_label_b = mlrPredict(W_b, validation_data)\n",
        "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label_b == validation_label).astype(float))) + '%')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------- Validation Data Set (Logistic Regression for Multiple Classification) -------------------\n",
            "\n",
            " Validation set of category 0 Accuracy: 97.8%\n",
            "\n",
            " Validation set of category 1 Accuracy: 97.6%\n",
            "\n",
            " Validation set of category 2 Accuracy: 90.10000000000001%\n",
            "\n",
            " Validation set of category 3 Accuracy: 90.10000000000001%\n",
            "\n",
            " Validation set of category 4 Accuracy: 94.0%\n",
            "\n",
            " Validation set of category 5 Accuracy: 88.9%\n",
            "\n",
            " Validation set of category 6 Accuracy: 95.6%\n",
            "\n",
            " Validation set of category 7 Accuracy: 92.5%\n",
            "\n",
            " Validation set of category 8 Accuracy: 86.2%\n",
            "\n",
            " Validation set of category 9 Accuracy: 92.60000000000001%\n",
            "\n",
            " Validation set Accuracy:92.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn0VzMLAcqGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da05e8e-385d-453c-a5da-6d7eac7a9a11"
      },
      "source": [
        "print('\\n------------------- Testing Data Set (Logistic Regression for Multiple Classification) -------------------')\n",
        "\n",
        "test_data_order_b = np.array([[test_data[j] for j in range(len(test_label)) if test_label[j] == i] for i in range(10)])\n",
        "#   separate each category\n",
        "\n",
        "for i in range(10):\n",
        "    test_data_order_b[i] = np.array(test_data_order_b[i])    \n",
        "    #   change the <class 'list'> to <class 'numpy.ndarray'>\n",
        "\n",
        "for i in range(10):\n",
        "    predicted_label_b = mlrPredict(W_b, test_data_order_b[i])\n",
        "    print('\\n Testing set of category ' + str(i) + ' Accuracy: ' + str(100 * np.mean((predicted_label_b == i).astype(float))) + '%')\n",
        "\n",
        "# Find the accuracy on Testing Dataset\n",
        "predicted_label_b = mlrPredict(W_b, test_data)\n",
        "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label_b == test_label).astype(float))) + '%')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------- Testing Data Set (Logistic Regression for Multiple Classification) -------------------\n",
            "\n",
            " Testing set of category 0 Accuracy: 98.16326530612245%\n",
            "\n",
            " Testing set of category 1 Accuracy: 97.79735682819384%\n",
            "\n",
            " Testing set of category 2 Accuracy: 89.53488372093024%\n",
            "\n",
            " Testing set of category 3 Accuracy: 90.89108910891089%\n",
            "\n",
            " Testing set of category 4 Accuracy: 93.48268839103869%\n",
            "\n",
            " Testing set of category 5 Accuracy: 86.54708520179372%\n",
            "\n",
            " Testing set of category 6 Accuracy: 94.98956158663883%\n",
            "\n",
            " Testing set of category 7 Accuracy: 92.02334630350194%\n",
            "\n",
            " Testing set of category 8 Accuracy: 88.50102669404517%\n",
            "\n",
            " Testing set of category 9 Accuracy: 91.97224975222993%\n",
            "\n",
            " Testing set Accuracy:92.5%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIBFTNYdcv8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424f3cf6-798b-4ab1-d574-94eead93d3b6"
      },
      "source": [
        "\"\"\"\n",
        "Script for Support Vector Machine\n",
        "\"\"\"\n",
        "\n",
        "print('\\n-------------- SVM -------------------\\n')\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# print(train_data.shape, train_label.shape, validation_data.shape, validation_label.shape, test_data.shape, test_label.shape)\n",
        "\n",
        "train_indices = np.random.permutation(train_data.shape[0])\n",
        "random_train_data = train_data[train_indices]\n",
        "random_train_label = train_label[train_indices]\n",
        "# print(random_train_data.shape, random_train_label.shape)\n",
        "random_train_data_10k = random_train_data[:10000]\n",
        "random_train_label_10k = random_train_label[:10000]\n",
        "# print(random_train_data_10k.shape, random_train_label_10k.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------- SVM -------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgwsBtLYc43M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b953d2e7-b39f-4f4b-cca8-fff562f50456"
      },
      "source": [
        "print('1. linear kernel')\n",
        "linear_kernel = svm.SVC(kernel = 'linear')\n",
        "linear_kernel.fit(random_train_data_10k, random_train_label_10k.flatten())\n",
        "print('training data accuracy: ' + str(100 * linear_kernel.score(random_train_data_10k, random_train_label_10k)) + '%')\n",
        "print('validation data accuracy: ' + str(100 * linear_kernel.score(validation_data, validation_label)) + '%')\n",
        "print('testing data accuracy: ' + str(100 * linear_kernel.score(test_data, test_label)) + '%')\n",
        "print('------------------------------------------------------------')\n",
        "\n",
        "print('2. radial basis function, gamma = 1.0')\n",
        "rbf_1 = svm.SVC(kernel = 'rbf', gamma = 1.0)\n",
        "rbf_1.fit(random_train_data_10k, random_train_label_10k.flatten())\n",
        "print('training data accuracy: ' + str(100 * rbf_1.score(random_train_data_10k, random_train_label_10k)) + '%')\n",
        "print('validation data accuracy: ' + str(100 * rbf_1.score(validation_data, validation_label)) + '%')\n",
        "print('testing data accuracy: ' + str(100 * rbf_1.score(test_data, test_label)) + '%')\n",
        "print('------------------------------------------------------------')\n",
        "\n",
        "print('3. radial basis function, gamma = default')\n",
        "rbf_default = svm.SVC(kernel = 'rbf', gamma = 'auto')\n",
        "\n",
        "# FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. \n",
        "# Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
        "# \"avoid this warning.\", FutureWarning)\n",
        "\n",
        "rbf_default.fit(random_train_data_10k, random_train_label_10k.flatten())\n",
        "print('training data accuracy: ' + str(100 * rbf_default.score(random_train_data_10k, random_train_label_10k)) + '%')\n",
        "print('validation data accuracy: ' + str(100 * rbf_default.score(validation_data, validation_label)) + '%')\n",
        "print('testing data accuracy: ' + str(100 * rbf_default.score(test_data, test_label)) + '%')\n",
        "print('------------------------------------------------------------')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. linear kernel\n",
            "training data accuracy: 99.63%\n",
            "validation data accuracy: 92.03%\n",
            "testing data accuracy: 91.5%\n",
            "------------------------------------------------------------\n",
            "2. radial basis function, gamma = 1.0\n",
            "training data accuracy: 100.0%\n",
            "validation data accuracy: 16.009999999999998%\n",
            "testing data accuracy: 17.93%\n",
            "------------------------------------------------------------\n",
            "3. radial basis function, gamma = default\n",
            "training data accuracy: 92.99%\n",
            "validation data accuracy: 92.13%\n",
            "testing data accuracy: 92.47999999999999%\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmOGm5cidQz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963d0d94-8299-4732-8868-19c3b30a1d91"
      },
      "source": [
        "print('4. radial basis function, gamma = default, varying value of C')\n",
        "\n",
        "training_data_score = np.zeros(11)\n",
        "validation_data_score = np.zeros(11)\n",
        "testing_data_score = np.zeros(11)\n",
        "c = np.array([1.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0])\n",
        "\n",
        "for i in tqdm(range(11)):\n",
        "    rbf_default_c = svm.SVC(C = c[i], kernel = 'rbf', gamma = 'auto')\n",
        "    rbf_default_c.fit(random_train_data_10k, random_train_label_10k.flatten())\n",
        "    training_data_score[i] = rbf_default_c.score(random_train_data_10k, random_train_label_10k)\n",
        "    validation_data_score[i] = rbf_default_c.score(validation_data, validation_label)\n",
        "    testing_data_score[i] = rbf_default_c.score(test_data, test_label)\n",
        "\n",
        "print('------------------------------------------------------------')\n",
        "\n",
        "for i in range(11):\n",
        "    print('C = ' + str(c[i]) + ', training data accuracy: ' + str(100 * training_data_score[i]) + '%')\n",
        "    print('C = ' + str(c[i]) + ', validation data accuracy: ' + str(100 * validation_data_score[i]) + '%')\n",
        "    print('C = ' + str(c[i]) + ', testing data accuracy: ' + str(100 * testing_data_score[i]) + '%')\n",
        "    print('------------------------------------------------------------')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. radial basis function, gamma = default, varying value of C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [13:24<00:00, 73.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "C = 1.0, training data accuracy: 92.99%\n",
            "C = 1.0, validation data accuracy: 92.13%\n",
            "C = 1.0, testing data accuracy: 92.47999999999999%\n",
            "------------------------------------------------------------\n",
            "C = 10.0, training data accuracy: 96.59%\n",
            "C = 10.0, validation data accuracy: 94.21000000000001%\n",
            "C = 10.0, testing data accuracy: 94.22%\n",
            "------------------------------------------------------------\n",
            "C = 20.0, training data accuracy: 97.76%\n",
            "C = 20.0, validation data accuracy: 94.59%\n",
            "C = 20.0, testing data accuracy: 94.72%\n",
            "------------------------------------------------------------\n",
            "C = 30.0, training data accuracy: 98.41%\n",
            "C = 30.0, validation data accuracy: 94.78999999999999%\n",
            "C = 30.0, testing data accuracy: 94.77%\n",
            "------------------------------------------------------------\n",
            "C = 40.0, training data accuracy: 98.75%\n",
            "C = 40.0, validation data accuracy: 94.85%\n",
            "C = 40.0, testing data accuracy: 94.76%\n",
            "------------------------------------------------------------\n",
            "C = 50.0, training data accuracy: 99.03%\n",
            "C = 50.0, validation data accuracy: 94.85%\n",
            "C = 50.0, testing data accuracy: 94.72%\n",
            "------------------------------------------------------------\n",
            "C = 60.0, training data accuracy: 99.27%\n",
            "C = 60.0, validation data accuracy: 94.86%\n",
            "C = 60.0, testing data accuracy: 94.62%\n",
            "------------------------------------------------------------\n",
            "C = 70.0, training data accuracy: 99.49%\n",
            "C = 70.0, validation data accuracy: 94.85%\n",
            "C = 70.0, testing data accuracy: 94.59%\n",
            "------------------------------------------------------------\n",
            "C = 80.0, training data accuracy: 99.63%\n",
            "C = 80.0, validation data accuracy: 94.85%\n",
            "C = 80.0, testing data accuracy: 94.59%\n",
            "------------------------------------------------------------\n",
            "C = 90.0, training data accuracy: 99.77000000000001%\n",
            "C = 90.0, validation data accuracy: 94.78999999999999%\n",
            "C = 90.0, testing data accuracy: 94.64%\n",
            "------------------------------------------------------------\n",
            "C = 100.0, training data accuracy: 99.82%\n",
            "C = 100.0, validation data accuracy: 94.73%\n",
            "C = 100.0, testing data accuracy: 94.72%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikHHq4AXdXcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "57f42b56-db64-4a8d-87ec-551048206b57"
      },
      "source": [
        "plt.plot(c, training_data_score, marker = 'o', color = 'g', label = 'Training Data Accuracy')\n",
        "plt.plot(c, validation_data_score, marker = 'v', color = 'r', label = 'Validation Data Accuracy')\n",
        "plt.plot(c, testing_data_score, marker = 'x', color = 'c', label = 'Testing Data Accuracy')\n",
        "\n",
        "plt.title('Accuracies of Radial Basis Function with Variable Values of C')\n",
        "plt.legend()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Values of C')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3wUdfrA8c+TSho1SE0CCCIgECSiAkpTAQV7gUPP9hP0sGA5T+FUQLHinZ56KiKnKGIHSRRQmqCIEroghh5CDaQAKaTs9/fHTMIm2SSbkM0m4Xm/XvvK9Hl2MzvPfsvMiDEGpZRSqir5eDsApZRSdY8mF6WUUlVOk4tSSqkqp8lFKaVUldPkopRSqsppclFKKVXlNLlUIRG5RET+rOZ99hGRbSJyQkSu9fC+2oiIERE/e3y+iNzu5rq7ReQyT8bnYp/jRWR6de6zKnjjOKrI/osfB9Wtqo47EekvIklVG51bMS0Tkf+r7v2WRkQ6ish6ETkuIg9W1XZrbHKx/wGpIhLo7VjcZYxZYYzpWM27nQy8aYwJNcbMLT7T/nJl2cnnoIh8ICKhVbFjY8xQY8yHp7sdO6YcO8bjIrJGRPpVQXzPG2Mq/CUu9pkVvFqebjxl7M+ISPuCcS8dR4WK7/90fhiIyBMistzF9HD7f35eJeKrkuOusjzxnrzscWCpMSbMGPMfVwuIyGARWW5/P5NF5EcRubqsjdbI5CIibYBLAAOU+QY8sG+v/Bo7DVHA5nKWGW6MCQWigR7Akx6PquJetmOsD7wNfC0ivl6MZ7idsAte+70YS232MdBbRNoWmz4C2GSM+d3dDYmlJpyzquw91RBlnkNE5EbgC2Am0BpoBjwNDC9rozXhH+XKX4FVwAdAkeKviESIyNd29jwqIm86zbtHRP6ws+sWETnfnl7kl6H9S/k5e7i/iCSJyD9E5CDwPxFpJCJx9j5S7eHWTus3FpH/ich+e/5c5205LddSRL6yt7PLucgpIr1EJF5EjonIIRH5V2kfhv2+totIiojMK/gVLSI7gHZArP3rusxSnjHmILAQK8kUbPsJEdnh9Jld5zTPV0SmisgREdkJXFUsrsLivYicLSJL7P/JERGZJSINy4qnlBgN8AnQGOsgLnfb9v9un/0e/hSRQfb0iSLysT1cT0Q+treRJiKrRaRZRWIr/gu+2PYLqopuF5FEO84JTsv6ilVNV/BZr7GP5YJfwBvs/+EtLo6jTvZnnSYim51/MdrH8lsi8q293V9F5OxS4v9QRB61h1vZ8Y51+oxTRMTHef8i8hEQyalj7HGnTY5y9V6dGWOSgCXAbcVm/RWY6cZ3bZmITBGRn4FMoF0ljrsL7GM71f7e1ivl8yn1+1qV76nYPguPIXu8eNVzAxF5X0QO2Mf4c2L/6BKR9mKVINLt9/6Zq33Yy15tHztp9ufXyZ6+BBgAvGn/f88ptp4A/wKeNcZMN8akG2McxpgfjTH3lLa/gg+qxr2A7cDfgJ5ALtDMnu4LbAD+DYQA9YC+9rybgH3ABYAA7YEoe54B2jtt/wPgOXu4P5AHvAQEAkFAE+AGIBgIw8rac53W/xb4DGgE+AP9nLaVZA/7AGuwMnwAVhLYCQy25/8C3GYPhwIXlfJZDASOAOfb8b0BLHeavxu4rIzPsnA+1q+OTcDrTvNvAlra8d4CZAAt7Hn3AluBCKyT/VL7s/Sz5y8D/s8ebg9cbsfYFFgOvOZOnMX+H772fncCvuVtG+gI7AVa2uNtgLPt4YnAx/bwGCDW/p/6Yh1b9cv7zMqaXmz7bezP5j2sY6g7cBLoZM//u/3Zd8Q6PrsDTUo5Pvtz6jjyx/o+jMc6jgYCx4GOTp/dUaAX4AfMAj4t5X3dBcTaw38BdgCfOc37pvj+S3nfZb5XF/sdBWxzGu8I5Nj/y/K+a8uARKCL/f78qfhx9zunjuGfKfrdd+v76oH3VBD/ROxjqNhnW/AdmwO8i3W+Owv4DRhjz5sNTLBjLzwXuoj1HKzv9eX25/c41jEVUDweF+uea8fTtsLn8Yqu4OkX0BcroYTb41uBh+3hi4Hkgg++2HoLgYdK2WZ5ySUHqFdGTNFAqj3cAnAAjVws53ywXggkFpv/JPA/e3g5MKngfZax7/exqowKxkPtz6eN05envORyAuuEZIDFQMMyll8PXGMPLwHudZp3BaUkFxfbuRZYVyyOspJLNpAGZNnDo8qIsXDbWCeXw8BlgH+x5SZy6uR/F7AS6ObGMVjwmaXZr7mu3gOuk0trp/m/ASPs4T8LPlc3jk/n4+gS4CDg4zR/NjDR6bOb7jTvSmBrKfs5G0jFOhm9g5VwC/bzIfBI8f2X8r7LfK8u9hsMHAN62+NTsBNZWd81p2NscrFlKnrcOR/DVwI7Kvp99cB7Kje5YJXcTwJBTvNHYrWPgFVNNc35/1DK/p8CPnca98H6Id7fjc+zjx1PqefH0l41sVrsduB7Y8wRe/wTTlWNRQB7jDF5LtaLwPolVhnJxpjsghERCRaRd0Vkj4gcw0oEDe3iaASQYoxJLWebUUBLuxiaJiJpWL8+C6pi7sb6RbHVrqIZVsp2WgJ7CkaMMSewfqm2qsD7u9YYE4b1ZToXCHd6r38Vq6dIQYznOc1viVUqKLCHUohIMxH51C66H8Oqlw4vbXkXphpjGmJ9aWOAV0RkaHnbNsZsB8ZhfUkP28u5anz/COsHyKdiVWe+LCL+ZcRzrTGmof2qSC+8g07DmVg/BqDyx2dLYK8xxuE0bQ9F//+l7bMIY8wOrF+w0VhJKw7YLyIdgX7AjxWMzd39ZmL9ev+rXc0yCuvEWN53rcDeEhu1uXncFT+GXR0f5X1fq/o9uSMKq6RxwCmmd7FKMGCVQAT4za7yuquU7RQ/hziwPhN3ziFH7b8tKhh7zUouIhIE3Az0E6tn00HgYaC7iHTH+kAixXWj+16sX2auZGKdtAo0LzbfFBt/FKuYe6Expj5waUGI9n4au6jXdRXPLqcTVENj9ca4EsAYs80YMxLrQHkJ+FJEQlxsZz/WQWYFYC3TBOuXR4UYY37E+qU71d5WFFbVxv1YVTQNsaoQxF7lANZJsUBkGZt/Hutz7Gp/Zrc6baciMRpjNYj+zKk2njK3bYz5xBjTF+tzMlifZ/Ht5hpjJhljOgO9gWFYdeQVkUHZx1FZyjo+y7IfiJCiDdmRVOL/b/sRuBGrSmSfPX47VhXv+lLWKf79qIwPsb7bl2NVFcXa08v6rrmzf3eOu+LHsKvOGWV+Xz3wngqUdUztxSq5hDvFVN8Y0wWsNlRjzD3GmJZYpdD/ilPbspPi5xDB+kzcOYb+tOO4wY1li6hRyQWrSJsPdMb6dRUNdAJWYJ0IfsM64b0oIiFiNdL2sdedDjwmIj3F0t4+eYL1pfmLWI2qQ7B+pZUlDKt6Jk1EGgPPFMwwxhwA5mP9IxuJiL+IXOpiG78Bx8VqbA6y932eiFwAICK3ikhT+1dEmr2Ow8V2ZgN3iki0WA32zwO/GmN2l/MeSvMacLmdrEOwvpjJdkx3YpVcCnwOPCgirUWkEfBEGdsNw6pKSheRVlhtDJUiIudiVY8W9GApddti9dEfaH822Vj/txKfo4gMEJGu9q/HY1hVi64+77KsB0bY//MYrJO0u6YDz4pIB/v47CYiTex5h7Dq+F35FevH0eP2fvtj9dL5tIKxF/gR68dEQUeCZfb4T8aY/FLWKSs+d63AOs6nYbUJ5djTS/2uucmd426sfQw3xmqjcNXwXeb31YPvaT1wqYhEikgDnHpy2uea74FXRaS+WJ0tzha7m76I3CSnOgqkYn2XXR3TnwNXicggu7T+KFbSWllGXAUxGOAR4CkRudMpjr4iMq2sdWtacrkdq44z0c7KB43Vw+lNrGKnYH2x2mM18iVhNUJjjPkCq97zE6z2hblYDXgAD9nrpdnbKXE9SDGvYTVUHsHqtbag2PzbsE5OW7Hq+8cV34D9RR2GlSB32duaDjSwFxkCbBaRE8DrWPXVWS62swirzvQrrMR6NlaXx0oxxiRjFd+fNsZsAV7F6lxwCOiKVWIo8B5WVdIGYC3wdRmbnoTV6SAdq8NDWcu68rhYvVUysL5Q/8OqAihv24HAi1if70GskqCrrtbNgS+xEssfWCfZjyoY41OcareYhHWsuetfWF/y7+0Y3sc6xsCq0vvQrvq42Xkl+4Q1HBiK9R7/C/zVGLO1grEX+BHr5FeQXH7C+uVc4roNJy8A/7Tje6wyO7VPUjOxfkHPdJpV3netPO4cd59gfe47saomn3MRX3nfV4+8J2PMD1jJbiNWh4K4Yov8FauDwRas4+5LTlVRXQD8ap9D5mG1Oe90sY8/sUp0b9gxDcfqap9TfNlSYvwS6zx7F1Yp6BDWZ/hNWeuJ3WijlFJKVZmaVnJRSilVB2hyUUopVeU0uSillKpymlyUUkpVudp2k8ZShYeHmzZt2ng7DKWUqlXWrFlzxBjTtKq3W2eSS5s2bYiPj/d2GEopVauISKl33jgdHqsWE5EZInJYRFzeftq+kOw/Yt3td6PYdzC2590u1gOwtombDwVSSilVc3iyzeUDrAsFSzMU6GC/RmM9wwOnK1ovxLrT6zP21eFKKaVqCY8lF2PMciCljEWuAWba95JahXVjtxbAYOAHY0zBzSF/oOwkpZRSqobxZm+xVhS9W2mSPa206UoppWqJWt0VWURGi/U0x/jk5GRvh6OUUsrmzeSyj6K3wm5tTyttegnGmGnGmBhjTEzTplXek04ppWq0WZtm0ea1NvhM8qHNa22YtWmWt0Mq5M3kMg/7QTsichGQbt9ieiFwhX07+0ZYTz9c6MU4lVKqxpm1aRajY0ezJ30PBsOe9D2Mjh1dYxKMx65zEZHZWE8+DBeRJKweYP4Axph3gO+wHjm6Het5FXfa81JE5Flgtb2pycaYsjoGKKWU18zaNIsJiyeQmJ5IZINIpgyawqiuoyq9PWMMOfk5ZORmkJmbSUZOhsvhhxY8RGZuZpF1M3MzmbB4wmntv6rUmVvux8TEGL2IUilVnWZtmsXoeaPJzDt1kg/0DeTOHncS3SzaSgi5GWTkZJwaLiNpFCyXX+pz28onCI5n3H8OnoisMcbEVHqHpagzV+grpVRVM8ZwJPMIiemJRV/HrL/x++NxmKIn8pP5J3kn/p0i0/x9/An2DyYkIIQQ/xBCAkII9g8mLCCM5qHNrXn+Ree5Gg7xDyncTv8P+rPveMnm6MgGZT2NvPpoclFK1QmVqZ7Kys0i6VhSqckjMT2R7LzsIusE+QUR2SCSyAaRJRJLAUFIeiSpMDH4+/pX2fss8NLlLzE6dnSRqrFg/2CmDJpS5fuqDE0uSqlar6Bxu+BEW9C4nZ6dTkzLmBLJY++xvSSmJ3I443CR7QhCi7AWRDaIJLp5NFefc3VhIolsEElEgwiaBDVBRABo81ob9qSXvDVXZINIWoa19Oh7LkicVdneU5W0zUUpVStl5GQUJomRX40kJav8fj8h/iFENYyykkX9yCKJI7JBJK3qtyLAN8DtGIonNbBKD9OGT6sxJ/nyaJuLUuqM4TAODp44WLK6yul1NOuoW9uaN2JeYfJoWK9hYamjKtT00oM3aclFKVVl3G33OH7yeKlVVYnpiSQdSyLXkVtknQaBDUqUNApeI74c4bJxO6pBFLvH7fbU260TtOSilKrRXLV73PXNXcRujaVJcJMijeRp2WlF1vUVX1rXb01kg0h6R/QukTwi6kfQoF6DUvdd0xu3z0SaXJRSlZaalcqGQxvYcHADE5ZMKHFRX05+Dp9t+YxG9RoR2SCSqAZRXBJ5SYnk0SK0Bb4+vpWOQ6unah6tFlNKlcthHOxI2cGGQxtYf3B9YULZe2xvuetW9KI+Vb20WkwpVS2OnzzOpsOb2HBwg5VEDm1g06FNZORmAFYVVsfwjvSN7Ev3Zt3p3rw70c2juWj6RaV2y1VnHk0uStUx7jaqG2NITE88VRKxSyM7UncULtOwXkO6N+vO3T3upnvz7nRv1p0uZ3Whnl+9EtubMmiKtnuoQppclKpDSruYMCcvh67NuhYpjWw8tLGwYV0Qzm58NtHNo7kj+o7CEklE/Qi3u+5qu4dypm0uStUhpV0x7izEP4RuzboVJpDuzbrTtVlXQgNCqylKVZNom4tSqoSjmUdZc2ANa/avIf5AfJmJ5cubvqR78+60a9QOH6nVD6FVtYAmF6VqiZSsFNbsX8OaA2uI3x/PmgNr2J22u3D+2Y3OJtg/uER3YLAuJryh8w3VGK0602lyUaoGSslKYe2BtYVJZM3+NexK21U4v12jdvRq1Yv7Yu6jZ4uenN/ifBoFNSr1XlfaqK6qmyYXpbwsNSu1SCKJ3x9fJJG0bdiWmJYxjOk5hpiWMYWJxBVtVFc1hUcb9EVkCPA64AtMN8a8WGx+FDADaAqkALcaY5LseS8BV9mLPmuM+aysfWmDvqpJSusOXJBI1hw4Vb21M3Vn4XptGrYhpmUMPVv0LEwkjYMae/GdqLrOUw36HksuIuILJACXA0nAamCkMWaL0zJfAHHGmA9FZCBwpzHmNhG5ChgHDAUCgWXAIGPMsdL2p8lF1RSuqqZ8xZcmQU04nHnq+SFRDaJKJJImwU28EbI6g9XG3mK9gO3GmJ0AIvIpcA2wxWmZzsAj9vBSYK7T9OXGmDwgT0Q2AkOAzz0Yr1Kn5cDxA/yU+BP3xd1XolE93+RzPOc4UwZOKUwk4cHhXopUKc/zZHJpBTjfeCgJuLDYMhuA67Gqzq4DwkSkiT39GRF5FQgGBlA0KQEgIqOB0QCRkXqLCVV9jDFsPbKVnxJ/4qe9P/FT4k9Fqrdcyc7LZvwl46spQqW8y9sN+o8Bb4rIHcByYB+Qb4z5XkQuAFYCycAvQH7xlY0x04BpYFWLVVfQ6syTk5/Dmv1rCpPJz4k/Fz6sqmlwU/pG9mXsBWPpG9mXm764icT0xBLb0HtsqTOJJ5PLPiDCaby1Pa2QMWY/VskFEQkFbjDGpNnzpgBT7HmfYLXfKFUt0rLT+GXvL4XJ5Ld9v5Gdlw1Ah8YduLrj1fSN7EvfyL50aNyhyC1Snh/0vHYHVmc8TyaX1UAHEWmLlVRGAH9xXkBEwoEUY4wDeBKr51hBZ4CGxpijItIN6AZ878FY1Rlub/peK5HYyWTToU0YDH4+fpzf4nzui7mPvpF96RPRh2ahzcrclnYHVsqDycUYkyci9wMLsboizzDGbBaRyUC8MWYe0B94QUQMVrXYWHt1f2CF/WvwGFYX5TxPxarqLlddgkeeN5LNhzcXaS8pqMYKDQild0Rvbux0I30j+9KrVS9CAkIqvN9RXUdpMlFnNL1xpaqzXHUJ9hEfAn0CycrPAqBlWEureivCquLq2qwrfj7ebopUqvrUxq7ISnlFQU+uB757oESXYIdx4Ovry0dXf0SfiD60adjG7VvKK6Xcp8lF1QnHTx5nya4lzN8+nwXbF5R5d+CMnAxu7XZrNUan1JlHk4uqlYwxbE7ezPxt85m/fT4/Jf5EriOX0IBQLmt3GeMvGc/kHyez7/i+Eutql2ClPE+Ti6o1jp08xqKdi5i/bT4Ldiwg6VgSAF3P6srDFz3M0A5D6R3RmwDfAABCAkK0S7BSXqLJRdVYxhg2HtpYWNX1896fyXPkUT+wPpe3u5yJ/SYyuP1gWtdv7XJ97RKslPdobzFVo6RlpxUpnew/vh+A7s26M7T9UIZ2GMrFrS/G39ffy5EqVTdobzFVa5V2+3mwem9tOLiB+duttpNf9v5CvsmnYb2GXN7ucoa2H8rg9oNpGdbSy+9CKVURmlyURxW/1mRP+h7umXcPq/au4njOcRZsX8ChjEMAnN/ifJ7o+wRD2w/lwtYX6vUmStVi+u1VHjVh8YQS15pk5WXx5uo3aVSvEYPbD2bI2UMY3H4wzUObeylKpVRV0+SiPMrV3YEBBCH578n4+vhWc0RKqerg4+0AVN10JPMI4xaMw+C6w0hkg0hNLErVYVpyUVUqMzeT11a9xks/v8SJnBP0j+rPr/t+JSsvq3AZvdZEqbpPSy6qSuQ58nh/7ft0eKMDE5ZMoH+b/my6bxNL71jKe1e/R1SDKAQhqkEU04ZP02tNlKrjtOSiTosxhriEOJ5Y/ARbkrdwUeuL+PSGT7kk6pLCZfT280qdeTS5qEpblbSKx394nBWJKzinyTl8dfNXXHfudXqXYaWUJhdVcQlHExi/eDxf/fEVzUKa8fZVb3N3j7v1qnmlVCFNLspth04cYtKPk5i2Zhr1/Ooxsd9EHu39KKEBod4OTSlVw3g0uYjIEOB1rMccTzfGvFhsfhQwA2gKpGA9zjjJnvcycBVWp4MfgIdMXbkRWi1zIucEU1dOZerKqZzMP8mYnmN4ut/T5T5LXil15vJYchERX+At4HIgCVgtIvOMMVucFpsKzDTGfCgiA4EXgNtEpDfQB+hmL/cT0A9Y5ql4VUm5+blMXzudST9O4lDGIW7sfCPPD3yeDk06eDs0pVQN58mSSy9guzFmJ4CIfApcAzgnl87AI/bwUmCuPWyAekAAIIA/cMiDsSonxhi+/uNrxi8ZT8LRBC6NupRvRnzDha0v9HZoSqlawpPXubQC9jqNJ9nTnG0ArreHrwPCRKSJMeYXrGRzwH4tNMb8UXwHIjJaROJFJD45ObnK38CZaMWeFfSe0Zsbv7gRPx8/5o2Yx7Lbl2liUUpViLcvonwM6Cci67CqvfYB+SLSHugEtMZKSANF5JLiKxtjphljYowxMU2bNq3OuOucLclbuHr21Vz6waUkpicyffh0Nty7geEdh2vXYqVUhXmyWmwfEOE03tqeVsgYsx+75CIiocANxpg0EbkHWGWMOWHPmw9cDKzwYLxnpP3H9/PM0meYsX4GoQGhPD/weR666CGC/YO9HZpSqhbzZHJZDXQQkbZYSWUE8BfnBUQkHEgxxjiAJ7F6jgEkAveIyAtYbS79gNc8GGudV/yBXf+85J/sStvFv1f9mzxHHg/2epAJl04gPDjc26EqpeoAjyUXY0yeiNwPLMTqijzDGLNZRCYD8caYeUB/4AURMcByYKy9+pfAQGATVuP+AmNMrKdiretcPrAr7h4ARp43kucGPke7Ru28GaJSqo6RunLpSExMjImPj/d2GDVSm9fasCd9T4npzUObc+DRA16ISClVU4jIGmNMTFVv19sN+qoalPbArkMntHe3UsozNLmcAZoENXE5PbJBZDVHopQ6U2hyqeM+2vARR7KO4CNF/9X6wC6llCdpcqnDZqybwe1zb2dQ20G8N1wf2KWUqj56V+Q6atqaaYyJG8Pgswcz55Y5BPkHcVePu7wdllLqDKEllzrozd/eZEzcGK7qcBVzR8wlyD/I2yEppc4wmlzqmH//8m8emP8A1557LV/f8jX1/Op5OySl1BlIk0sd8vLPL/PI949wY+cb+fzGzwnwDfB2SEqpM5QmlzriueXP8Y9F/2DkeSOZfcNsfeSwUsqrNLnUcsYYnl76NE8tfYrbut3GR9d9hJ+P9tNQSnmXnoVqMWMM4xeP58WfX+TuHnfz7rB38fXx9XZYSimlyaW2Msbw9x/+zqu/vMq9Pe/lraveKnGhpFJKeYsml1rIGMNDCx7ijd/e4IFeD/D6kNf1gV5KqRpFk0st4zAOxn47lnfWvMMjFz3C1CumamJRStU4mlxqEYdxMDp2NO+ve58n+jzB84Oe18SilKqRNLnUEvmOfO6adxczN8zkqUufYlL/SZpYlFI1liaXWiDPkcdf5/yV2b/PZnL/yTzV7ylvh6SUUmXyaPciERkiIn+KyHYRecLF/CgRWSwiG0VkmYi0tqcPEJH1Tq9sEbnWk7HWVLn5uYz8aiSzf5/Ni4Ne1MSilKoVPJZcRMQXeAsYCnQGRopI52KLTQVmGmO6AZOBFwCMMUuNMdHGmGhgIJAJfO+pWGuqnPwcbv7yZr7c8iWvXvEq/+j7D2+HpJRSbvFkyaUXsN0Ys9MYkwN8ClxTbJnOwBJ7eKmL+QA3AvONMZkei7QGys7L5vrPrmfu1rm8MfQNHrn4EW+HpJRSbvNkcmkF7HUaT7KnOdsAXG8PXweEiUjxZ/KOAGa72oGIjBaReBGJT05OroKQa4as3Cyu/fRavt32Le9c9Q7397rf2yEppVSFePuS7seAfiKyDugH7APyC2aKSAugK7DQ1crGmGnGmBhjTEzTpk2rI16Py8zNZPjs4Xy/43vev/p9xsSM8XZISilVYZ7sLbYPiHAab21PK2SM2Y9dchGRUOAGY0ya0yI3A3OMMbkejLPGOJFzgmGfDGNF4go+vPZDbut+m7dDUkqpSvFkyWU10EFE2opIAFb11jznBUQkXKTwhlhPAjOKbWMkpVSJ1TXHTh5jyMdD+CnxJz6+7mNNLEqpWs1jycUYkwfcj1Wl9QfwuTFms4hMFpGr7cX6A3+KSALQDJhSsL6ItMEq+fzoqRhrirTsNK746Ap+3fcrn974KSO7jvR2SEopdVrEGOPtGKpETEyMiY+P93YYFZaSlcLgjwez4eAGPr/pc64994y8nEcp5SUissYYE1PV2y235CIiw52qrlQVOpJ5hEEzB7Hx0Ebm3DJHE4tSqs5wp0H/FuA1EfkKmGGM2erhmOqsWZtmMWHxBBLTE2lVvxUYOJJ1hHkj5jG4/WBvh6eUUlWm3ORijLlVROpjNa5/ICIG+B8w2xhz3NMB1hWzNs1idOxoMnOta0GTjiUB8GSfJzWxKKXqHLequ4wxx4Avsa6yb4F1weNaEXnAg7HVKRMWTyhMLM4++f0TL0SjlFKe5U6by9UiMgdYBvgDvYwxQ4HuwKOeDa/uSExPrNB0pZSqzdxpc7kB+LcxZrnzRGNMpojc7Zmw6p7IBpHsSd/jcrpSStU17lSLTQR+KxgRkSD7GhSMMYs9ElUdNGXQFIL8gopMC/YPZsqgKaWsoZRStZc7yeULwOE0nm9PUxUwqusobu5yMwCCENUgimnDpzGq6ygvR6aUUlXPnWoxP/uW+QAYY3Ls27moCjqUcYj2jduTcH+CPqJYKVWnuVNySXa6XQsicg1wxIiQ1YkAACAASURBVHMh1U0nck6wZNcShp8zXBOLUqrOc6fkci8wS0TeBATrGS1/9WhUddCinYvIyc9h2DnDvB2KUkp5nDsXUe4ALrJviY8x5oTHo6qDYv+MpX5gfS6JvMTboSillMe59TwXEbkK6ALUK6jSMcZM9mBcdYrDOPh227cMaT8Ef19/b4ejlFIe585FlO9g3V/sAaxqsZuAKA/HVafE74/nUMYhhp8z3NuhKKVUtXCnQb+3MeavQKoxZhJwMXCOZ8OqW+IS4vARH4a2H+rtUJRSqlq4k1yy7b+ZItISyMW6v5hyU2xCLL0jetMkuIm3Q1FKqWrhTnKJFZGGwCvAWmA34NbdFkVkiIj8KSLbReQJF/OjRGSxiGwUkWUi0tppXqSIfC8if4jIloK7AtQ2SceSWH9wvVaJKaXOKGU26NsPCVtsjEkDvhKROKCeMSa9vA2LiC/wFnA5kASsFpF5xpgtTotNBWYaYz4UkYHAC0DBw+NnAlOMMT/YPdWc7xJQa8QlxAFoF2Sl1BmlzJKLMcaBlSAKxk+6k1hsvYDtxpid9hX+nwLXFFumM7DEHl5aMF9EOmPdGeAHe78njDEl71dfC8QlxNGuUTs6hXfydihKKVVt3KkWWywiN0jFLytvhXXBZYEke5qzDcD19vB1QJiINMHqMJAmIl+LyDoRecUuCRUhIqNFJF5E4pOTkysYnudl5mayeNdihnUYplflK6XOKO4klzFYN6o8KSLHROS4iByrov0/BvQTkXVAP2Af1o0x/YBL7PkXAO2AO4qvbIyZZoyJMcbENG3atIpCqjqLdi4iOy+b4R21vUUpdWZx5wr9sEpuex8Q4TTe2p7mvO392CUXu13lBmNMmogkAeuNMTvteXOBi4D3KxmLV8QlxBEWEMalUZd6OxSllKpW5SYXEXF5Ziz+8DAXVgMdRKQtVlIZAfyl2LbDgRS7bedJYIbTug1FpKkxJhkYCMSXF2tN4jAO4hLiGNx+MAG+ehNppdSZxZ3bv/zdabgeVkP9GqwTfqmMMXkicj+wEPAFZhhjNovIZCDeGDMP6A+8ICIGWA6MtdfNF5HHsNp7xN7fexV6Z1627sA6Dpw4wLAO2ktMKXXmcadarEiDgYhEAK+5s3FjzHfAd8WmPe00/CXwZSnr/gB0c2c/NVFsQiyCcGWHK70dilJKVTt3GvSLSwK0X2054hLiuDjiYpqG1LyOBkop5WnutLm8ARh71AeIxrpSX5Vi//H9rDmwhucHPu/tUJRSyivcaXNxbkjPA2YbY372UDx1wrcJ3wJoF2Sl1BnLneTyJZBtjMkH67YuIhJcW6+Yrw6xCbFENYiiS9Mu3g5FKaW8wq0r9IEgp/EgYJFnwqn9snKzWLRzEcPPGa5X5SulzljuJJd6zo82toeDPRdS7bZk1xKy8rL0RpVKqTOaO8klQ0TOLxgRkZ5AludCqt3iEuII8Q+hf5v+3g5FKaW8xp02l3HAFyKyH+sxx82xHnusijHGELctjivOvoJAv0Bvh6OUUl7jzkWUq0XkXKCjPelPY0yuZ8OqndYfXE/SsSQm95/s7VCUUsqryq0WE5GxQIgx5ndjzO9AqIj8zfOh1T5xCXF6Vb5SSuFem8s99pMoATDGpAL3eC6k2is2IZZerXrRLLSZt0NRSimvcie5+Do/KMx+aJfe5reYgycOsnr/au0lppRSuNegvwD4TETetcfHAPM9F1LtVHhV/jl6Vb5SSrmTXP4BjAbutcc3YvUYU07itsURUT+Cbs1q7Y2clVKqypRbLWY/yOtXYDfWs1wGAn94NqzaJTsvm+93fM+wc4bpVflKKUUZJRcROQcYab+OAJ8BGGMGVE9otcey3cvIzM3UKjGllLKVVS22FVgBDDPGbAcQkYerJapaJvbPWIL9gxnQVvOuUkpB2dVi1wMHgKUi8p6IDMK6Qt9tIjJERP4Uke0i8oSL+VEislhENorIMhFp7TQvX0TW2695FdlvdSq4Kv/ydpdTz6+et8NRSqkaodTkYoyZa4wZAZwLLMW6DcxZIvK2iFxR3obtLstvAUOBzsBIEelcbLGpwExjTDdgMvCC07wsY0y0/bq6Qu+qGm06vInE9ETtgqyUUk7cadDPMMZ8YowZDrQG1mH1ICtPL2C7MWanMSYH+BS4ptgynYEl9vBSF/NrvLiEOACu6nCVlyNRSqmaw52LKAsZY1KNMdOMMYPcWLwVsNdpPMme5mwDVvUbwHVAmIg0scfriUi8iKwSkWtd7UBERtvLxCcnJ1fgnVSd2IRYYlrG0CKshVf2r5RSNVGFkosHPAb0E5F1QD9gH5Bvz4syxsQAfwFeE5Gzi69sJ7oYY0xM06ZNqy3oAoczDvNr0q/aS6ym6tEDREq+evSom/vVfXtn38olTyaXfUCE03hre1ohY8x+Y8z1xpgewAR7Wpr9d5/9dyewDKhxR8l3277DYLS9paa6+GIIKHanooAA6N27bu5X9+2dfSuX3LlCv7JWAx1EpC1WUhmBVQopJCLhQIp9oeaTwAx7eiMg0xhz0l6mD/CyB2OtlNiEWFqFtaJH8xqX92qMlx99lAuWLGHA+vWF05ZGR7N64EAef/VV9zfkcEBGBpw4AcePl3y5mP5yRAQXnHceA9auPbXv885jdWgoj/fvX4XvspiTJyEvr+i0vDxYtQp69bLei6tXfn7F5xWfnp8PxhTdd04OvPsuTJ8OPj6nXr6+RcdPd15enuv3vW4dePLzBtefeX6+Ne3ppyEszPUrNLTosE8Ff2/36AFOx3ah6GjrfZ/BPJZcjDF5InI/sBDwBWYYYzaLyGQg3hgzD+gPvCAiBlgOjLVX7wS8KyIOrNLVi8aYLZ6KtTJO5p3k+x3fM6rrqBp/Vf7LiYlcEBbGgEaNCqctTU1l9fHjPB4Z6dF9XxAays3PPMPnkyYxYP16lkZHW+Nz58K//uV2oiAjo+RJszQhIRAaygUxMdz89NPWvtetY2mPHta+v/7ao+/55YEDuSAkhAFLllgxi7B04EBWX3wxj69eXfUn9eLz5s+3Tnj5+db06GgYPPj0E5k785o3hwMHCt83zZuXLFF4QmAgNGsGBw+eOk4CA+GTT6zjyV0hISWTTmnJKCwMmjQBP7+iiS0gAGJirM+kosmqAqrsh5uHiHH3C1vDxcTEmPj4+Grb3/c7vmfwx4OJHRlb46vFlqamcvOWLXzeuTMDGjUqMe6OPIeDDIeDjPx8MvPzC4cz8vPJLBjOySHz4EEyDh8m48gRMlNTyTh2jJ0hIazo1o02Bw+yu3lzLtyyhWapqfgYg6/DgY+PDz6+vtbLz6/w5evnh4+/vzXu72+9AgLwtf8WeQUG4hMYiG9AgLU9EXyAnUeP8tHBg1y6YQMrunVjXEQEMS1aUM/HhyAfnxJ/g3x9qWeP+1byR8PS1FRu/v13Pn/sMQb89htLL7yQm195hc/PO8/tz/u0HDgA7dpBdjYEBcHOndZJvjrUxH2XVeqtQEm4cHpFklVoqOtE5W7ycp4eElIkWS195hlu7tGj5A+39esZMHGi2yGKyBq7fbtKaXKppAfnP8j0tdM5+vhRgvyDqm2/lbUkJYUbtmyhV1gYP6Wnc114OE0DAqxE4ZQsMksZzq3gceKbn0/IyZMEG0NIVhbHfX053KgRZ6WkcJYxOFq0IF8Eh48PDmNwQJG/+cXGC6e7mOYAPHEU+4uUTEB28glylZyc5u0/eZJPk5LoHx/Pip49efbccxnSpAlN/f1p6Ofn+dLu3/5mVYXdey+89ZZn93Wm7bsgWR0/zssHDnDBRx8x4K23rNKLnx9LR41i9WWX8fi2baUnKedklZHh1m6NCJmNG5PavDkpZ51FasOGLA8L4+VbbmHIb7+xLDqaz198kQFz51YooWtyKUd1JhdjDO3+047zzjqP2JGx1bLPysrIz2fWoUO8tW8fG50OYh8gxNeXEF9fgn18XA8DIcePE3L0KCGHDxO8bx8hiYmEJCYSnJFBSHY2IdnZBIeHExIRQUjbtgR36EBIp04EnHsuEhwMwNIdO7j599+575tvePuaa6xf8GeX6Px3WkwpSWdZaiq3b9nCX3/6iQ/79uXVDh3oFhpKtsNBlsNh/c3PLxwvnOZiXollyphXFj8Rmvr709Tfn7MCAkoMn+XvT1On4QYVSEaFVaDZ2TBiBHz2GUsDA6ulCrTQgQOF+662UouX9l3RUqoxhkyHg9TcXFLz8qxXTg6pGRnWKyuL1JMnSSmYb4z18vEh1deX3DKq2Z6aNYvJAQEVTqqeSi6ebNCvs7Ykb2F32m6e7Pukt0MpVUJmJv/dt48PDh4kPT+fs/ftI7RRI0bHxjJz8GA+mzSJgWA1OublwfbtsHmz9dqyxfr755+Qm2ttUATatoUuXaB7d+tvly5w7rlgJxFXlqamcvPBg3y+di0DPvyQAZGR3BwezueNG1dpFZGI4AtFqrKWpqZyx59/8kXXrgy49FKGV6I6sDKMMXyfksKoP/7g1mbN+PDQIZ6MjKRlQADJubkczs0lOSfH+puby66sLA7n5nI8P9/l9vydklHTgAAr+ZSSmDoFB596jz/+WKQK1NMKE1uLFvDjj0D1te15et/GGE7aVcPOpX0/ER6LjOTa556j/2+/sfjCC7k+PJyvjxzh/QMHTiWQvLzChJJTxg96ARr6+dEoKIhGfn408vcnws+PxvZwIz+/Uy9/f7YfPMj4bdu4+9tveXvYMAacdx415Q6HmlwqITbBKq3UtKvy840h7uhR3tq3jx9SU/EX4camTbl4wQImt2vHvAkTGLB+PcNWrbLqZt97jwHdullJJCfn1IYKksiVVxZNIiEhFY5p9fHj1omuXTtYtowB997L5/YvaU+3PxTu297PgEaN+LxzZ4/ve1laGrdu3coXXbowoFEjrgkPLzzB31rGr+ns/HyS7YSTnJvL4ZycIn8LhnfayehEKcnIF7hswwYa+/mRmpdHh6Agntm9m+cTEwkQIdDHhwARAnx8CoeL/PXxIdCe72pa8XUKprUOCODGzZt5v2NH+jVsyIq0NO76809mdOxIasGPFA85NyiImzZvZnrHjsSEhbE4LY1x27czKSqKhSkpJdsHiw2XOc9OKGWWR/39mdenDwAfHzpEAzsBFCSF1oGBRZKCc5Jo7DRe388PHzdLqUtTU/lnSgpfrlnDgOnTGdqihUd+uFWWVotVQt8ZfcnKy2LN6DXVsr/yJOfkMP3AAd7Zv5/EkydpHRjImBYtuKdlS5oZw8vz53PBs88W7ZIbHc3qCy/k8aQkK3l07mz97dSpUklEnVJdvfMKklHxklByTg4LUlLYkJFBh6Ag2tarR479yzvH4Tg17DTtpDGF884UASIE+/oSYlcFOw+XVV1cfHxrRgZP7d7NqGbNmH3oEJ917sygxo09Hn9VVYFqm0s5qiu5HMk8QrOpzfjnJf9k0oBJHt9faYwx/HrsGG/t38/nhw+TYwwDGzZkbKtWXB0YiN+CBTBnDnz7rdVw6O9/qhupvz/cfju8957X4leeU1AVdl/Llry9f3+FqgGNMeTaiaYg4ZwslpBcTXNOXF8nJ7MwNZUrGjXiyiZNyt9pFZp/9CgLU1O5Ljyc25s3L5oUfHysBGInB/8q6CZcFT0xvU3bXGqI77Z9h8M4GN7RO7d8ycrPZ/bhw7y1bx9rT5wgzNeX0S1b8regIDp9/z089RT88IN1UVnTpnDLLXDddVbJpFMnq5umnx88+6xX4leeVfzkNqBhwwqd7ESksKortJL7X3PiBE9FRfH2/v08ERlZbSfZpampPLdnT+G+H2jVqs5WvdYGmlwqKC4hjuahzTm/xfnVut8dWVm8vW8fMw4eJDUvjy7Bwfw3PJxbly0j7OmnYflyq1QSFQX33WcllD59rIvoCtx5p9VN8847q78Xj6oW3jzZnW5iq437dlX9NKBRozM+sYBWi1VITn4O4S+Hc0uXW3jvas9XKTmMYX5KCm/t28eClBR8gOsDAxm7Zg2XfvghUvB+O3eG66+3EkrBDfxc8WYXUVXnefNOEN7cd22nbS7lqI7ksnjnYi776DK+GfENV3f03PPLjubmMuPAAd7ev59d2dk0F2HM9u2Mfu89Wv76q7VQr16nEso553gsFqVU3aZtLjVAbEIsgb6BDGrrzuNsKi7ebqD/9PBhsh0OLjlyhBc++4zr5s4lwBjo1w/efBOuuQZaty5/g0op5SWaXNxkjCE2IZZB7QYRElDxrrql3WRu5WWXEfHoo7y1dy+/ZWQQkpvL7UuXMvbTT+l64IB1w8Hp02HYMOsmeUopVQtocnHT1iNb2Zm6k8cufqxS6xe/O/DsAQO4+/HH8TeGY1u30jEpidfnzOH2lStpMGgQvPIKDBmi15wopWolTS5uikuIA6j0HZAH3Hsvn197LTdOmkR4ejoJrVsjDgeDV65k7I8/Mqh9e2T0aJg9u3puUa6UUh6kycVNsQmxdG/WnYgGEeUv7EqLFvTv2ZP6GRkkRETQZ+NGPvn6ayKnTIHx44t2GVZKqVrOk485rjNSslL4ee/PDD/n9C6c/Odtt7G7RQuGrlrFn1FR7Hj//ZLXoiilVB2gycUN87fNx2Ecp/VQsHnJybyQnc25u3cTN2ECn69bx80HD7I0NbUKI1VKqZrBo8lFRIaIyJ8isl1EnnAxP0pEFovIRhFZJiKti82vLyJJIvKmJ+MsT9y2OM4KOYsLWl1Q6W08v20bOBzMXrsWn759rTYY+8pppZSqazzW5iIivsBbwOVAErBaROYZY7Y4LTYVmGmM+VBEBgIvALc5zX8WWO6pGN2Rm5/L/G3zub7T9fhI5XLxL+np/JqTw8Nz5xL9+OMQYbXbDAC9TYRSqk7yZMmlF7DdGLPTGJMDfApcU2yZzsASe3ip83wR6Qk0A773YIzl+inxJ9JPple6vSXX4eDe33+n9eHDTAoIKEwsSilVl3kyubQC9jqNJ9nTnG0ArreHrwPCRKSJiPgArwJlXlQiIqNFJF5E4pOTk6so7KLiEuII8A3g8rMvr9T6/9m3j425ufxn2jTC/v73Ko5OKaVqJm836D8G9BORdUA/YB+QD/wN+M4Yk1TWysaYacaYGGNMTNOmTT0SYGxCLAPaDCA0oOI3IE/MzuaZHTsYvnIl18bE6M0ilVJnDE9e57IPcK4Dam1PK2SM2Y9dchGRUOAGY0yaiFwMXCIifwNCgQAROWGMKdEpwJMSjiawLWUbD134UKXWf3DbNkxuLm/MmIH89lsVR6eUUjWXJ5PLaqCDiLTFSiojgL84LyAi4UCKMcYBPAnMADDGjHJa5g4gproTC0Dsn7FA5a7K/+bIEb45epSXZswg6rbboBoee6qUUjWFx6rFjDF5wP3AQuAP4HNjzGYRmSwiBfer7w/8KSIJWI33UzwVT2XEbYuj61ldiWoYVaH1TuTl8cC2bZyXnMzDixfDww97KEKllKqZPHr7F2PMd8B3xaY97TT8JfBlOdv4APjAA+GVKTUrlRV7VvB4n8crvO6kPXvYe/IksydPxv+xx6B+fQ9EqGqb3NxckpKSyM7O9nYo6gxUr149Wrdujb+/f7XsT+8tVoqFOxaSb/Ir3AV544kT/HvvXv5v7Vr6JCfD/fd7KEJV2yQlJREWFkabNm2Q0p4WqpQHGGM4evQoSUlJtG3btlr26e3eYjVWbEIs4cHh9GrVy+11HMYwJiGBRsbw0sSJMGECBAd7LkhVq2RnZ9OkSRNNLKraiQhNmjSp1lKzllxcyHPkMX/bfK7ueDW+Pu7fVHL6gQOsOnaMD7/4gsYNG8Lo0R6MUtVGmliUt1T3safJxYWVe1eSmp1aoV5ih3Ny+MfOnfTPy+O2t9+G996DwEAPRqmUUjWXVou5EJcQh7+PP1ecfYXb6zy6YwcZ+fm8/fLLyNlnw+23ezBCdSaYtWkWbV5rg88kH9q81oZZm2ad1vaOHj1KdHQ00dHRNG/enFatWhWO5+TklLlufHw8Dz74YLn76N2792nFWGDZsmU0aNCAHj160LFjRy699FLi4uLcWm/lypWV2ue4ceNo1aoVDoejUuurorTk4kJsQiz92/SnfqB7vbyWpKby8aFD/DMjg3N/+AE+/hiqqUeGqptmbZrF6NjRZOZmArAnfQ+jY61q1lFdR5W1aqmaNGnC+vXrAZg4cSKhoaE89tipOyzl5eXh5+f6lBATE0NMTEy5+6jsid2VSy65pDChrF+/nmuvvZagoCAGDRpU6jrLli0jNDS0wknO4XAwZ84cIiIi+PHHHxkwYMBpxV6asj7juubMeJcVsD1lO1uPbOW+mPvcWv6kw8F9CQmcXa8e4x98EDp3hhEjPBylqu3GLRjH+oPrS52/KmkVJ/NPFpmWmZvJ3d/czXtr3nO5TnTzaF4b8lqF4rjjjjuoV68e69ato0+fPowYMYKHHnqI7OxsgoKC+N///kfHjh1ZtmwZU6dOJS4ujokTJ5KYmMjOnTtJTExk3LhxhaWa0NBQTpw4wbJly5g4cSLh4eH8/vvv9OzZk48//hgR4bvvvuORRx4hJCSEPn36sHPnznJLJdHR0Tz99NO8+eabDBo0iNjYWJ577jlycnJo0qQJs2bNIisri3feeQdfX18+/vhj3njjDdLS0kos16xZsxLbX7ZsGV26dOGWW25h9uzZhcnl0KFD3HvvvezcuROAt99+m969ezNz5kymTp2KiNCtWzc++ugj7rjjDoYNG8aNN95Y4rN46qmnaNSoEVu3biUhIYFrr72WvXv3kp2dzUMPPcRou312wYIFjB8/nvz8fMLDw/nhhx/o2LEjK1eupGnTpjgcDs455xx++eUXPHXLq6qiyaWYuATrIHe3veWlxEQSsrJYcPgwQZs2wVdf6ZMl1WkrnljKm346kpKSWLlyJb6+vhw7dowVK1bg5+fHokWLGD9+PF999VWJdbZu3crSpUs5fvw4HTt25L777itx/cS6devYvHkzLVu2pE+fPvz888/ExMQwZswYli9fTtu2bRk5cqTbcZ5//vm88sorAPTt25dVq1YhIkyfPp2XX36ZV199lXvvvbdIiSw1NdXlcsXNnj2bkSNHcs011zB+/Hhyc3Px9/fnwQcfpF+/fsyZM4f8/HxOnDjB5s2bee6551i5ciXh4eGkpKSUG/vatWv5/fffC7sBz5gxg8aNG5OVlcUFF1zADTfcgMPh4J577in8bFJSUvDx8eHWW29l1qxZjBs3jkWLFtG9e/can1hAk0sJsQmxdG7amXaN2pW77LbMTJ7fs4dbmjRh8F13wfnnw3XXVUOUqrYrr4TR5rU27EnfU2J6VIMolt2xrEpjuemmm/C1fxClp6dz++23s23bNkSE3Nxcl+tcddVVBAYGEhgYyFlnncWhQ4do3brIs/7o1atX4bTo6Gh2795NaGgo7dq1KzzJjhw5kmnTprkVpzGmcDgpKYlbbrmFAwcOkJOTU+q1G+4sl5OTw3fffce//vUvwsLCuPDCC1m4cCHDhg1jyZIlzJw5EwBfX18aNGjAzJkzuemmmwgPDwegsRu3durVq1eRff/nP/9hzpw5AOzdu5dt27aRnJzMpZdeWrhcwXbvuusurrnmGsaNG8eMGTO488473fm4vE4b9J2kZ6ezfM9yty6cNMYwdts2An18+Pevv8KuXfDcc6BdTVUVmDJoCsH+Ra+RCvYPZsqgqr9DUkhISOHwU089xYABA/j999+JjY0t9bqIQKeekL6+vuTl5VVqmYpYt24dnTp1AuCBBx7g/vvvZ9OmTbz77rulxunOcgsXLiQtLY2uXbvSpk0bfvrpJ2bPnl3h+Pz8/Ao7AzgcjiKdJJw/42XLlrFo0SJ++eUXNmzYQI8ePcq8/iQiIoJmzZqxZMkSfvvtN4YOHVrh2LxBk4uThTsWkufIc6tK7NPDh/khNZXnIyJoMXEi9O4NQ4Z4Pkh1RhjVdRTThk8jqkEUghDVIIppw6dVujHfXenp6bRqZT126YMPPqjy7Xfs2JGdO3eye/duAD777DO31tu4cSPPPvssY8eOLRHnhx9+WLhcWFgYx50eHV7acs5mz57N9OnT2b17N7t372bXrl388MMPZGZmMmjQIN5++20A8vPzSU9PZ+DAgXzxxRccPXoUoLBarE2bNqxZswaAefPmlVrqS09Pp1GjRgQHB7N161ZWrVoFwEUXXcTy5cvZtWtXke0C/N///R+33nprkVJmTafJxUlcQhyNgxpzceuLy1wuLTeXh7dvJyYsjHvnzoV9+2DKFC21qCo1qusodo/bjeMZB7vH7fZ4YgF4/PHHefLJJ+nRo8dplzRcCQoK4r///S9DhgyhZ8+ehIWF0aBBA5fLrlixorAr8tixY/nPf/5T2FNs4sSJ3HTTTfTs2bOwegpg+PDhzJkzh+joaFasWFHqcgUyMzNZsGABV111VeG0kJAQ+vbtS2xsLK+//jpLly6la9eu9OzZky1bttClSxcmTJhAv3796N69O4888ggA99xzDz/++CPdu3fnl19+KVJacTZkyBDy8vLo1KkTTzzxBBdddBEATZs2Zdq0aVx//fV0796dW265pXCdq6++mhMnTtSaKjEAca7HrM1iYmJMfHx8pdfPd+TTbGozhnYYykfXfVTmsmMTEnhn/35Wd+rE+V27QrdusGhRpfetzgx//PFHYbXOmezEiROEhoZaVctjx9KhQwce1juHlyk+Pp6HH36YFStWnNZ2XB2DIrLGGFN+P/MK0pKLbVXSKo5mHS23veW3Y8d4e/9+HmjVivNnzIDkZKutRSnllvfee4/o6Gi6dOlCeno6Y8aM8XZINdqLL77IDTfcwAsvvODtUCpESy62JxY9wau/vMqRvx+hQT3XxfQ8h4ML1q4lOSeHPzp2JKx9e+jbF2JjK71fdebQkovytuosuWhXZFtc55eDDAAAGTRJREFUQhyXRl1aamIBeGPfPtafOMGXXboQ9vrrkJYGzz5bjVEqpVTt4NFqMREZIiJ/ish2ESnxmGIRiRKRxSKyUUSWiUhrp+lrRWS9iGwWkXs9Geeu1F1sTt5cZpXY3uxsntq1iysbN+Z6Y+Df/4abboLoaE+GppRStZLHkouI+AJvAUOBzsBIEelcbLGpwExjTDdgMlBQqXgAuNgYEw1cCDwhIi09EeesTbPoOa0nAK+sfKXUmwOO274dB/Bmhw7IK69AZiZMmuSJkJRSqtbzZMmlF7DdGLPTGJMDfApcU2yZzsASe3hpwXxjTI4xpuA+F4GeirPg5oCp2akA7D++n9Gxo0skmLgjR/j6yBGejoqibWoqvPkm3HoraP25Ukq55Mnk0grY6zSeZE9ztgG43h6+DggTkSYAIhIhIhvtbbxkjNlf1QFOWDyh8K6zBTJzM5mweELheEZ+Pvdv20bn4GAeiYiA55+HvDx45pmqDkepU3r0sK6bKv7q0aPSmxwwYAALFy4sMu21117jvvtKv0lr//79Kegoc+WVV5KWllZimYkTJzJ16tQy9z137ly2bNlSOP7000+zqAq67+ut+Wsub3dFfgzoJyLrgH7APiAfwBiz164uaw/cLiIlbmUqIqNFJF5E4pOTkyu888T0xHKnP7t7N3tOnuSdc84hIDERpk2Du++GduXfe0ypSrv4YggIKDotIMC6E0QljRw5kk8//f/27j0sqnrf4/j7K4L3AC9bMTWtvIEy4LWzlYLSpzr1qGiRZTvN0/ZBu5lWW4+WWek2L7t0Z9LFS7UNNSs1tTpplpbtAhUQSUmL1DRCN5qEouj3/LHWTAOCIs4Izvxez8PDzJq11vyWC/nyW79Zn9+SEsuWLFlS4fDItWvXEhISUqn3Ll1cnn32Wfr06VOpfZUWExPDtm3b2LVrF3PmzOGhhx5i/fr159ymssWldDS/t3jjBtZLzZvF5WegpdvzFvYyF1U9oKoDVTUamGAvO1J6HSATiCn9Bqr6mqp2U9VulUkJbRXc6pzLMwsKmLV/P8ObNSMmJMT6ZFiNGjBx4gW/l2GUMHo0xMaW/7Vtm9VDdldcbC0vb5vRo8/5lnfccQdr1qxxZV7l5ORw4MABYmJiGDlyJN26dSMiIoJJ5fTKW7duzaFDhwCYMmUK7dq1o3fv3uzatcu1zuuvv0737t1xOBwMGjSIwsJCNm/ezKpVq3jiiSeIiopiz549DBs2jOXLlwOwfv16oqOj6dy5M8OHD6eoqMj1fpMmTaJLly507tyZnTt3nvef1T2aH+DDDz+kZ8+eREdH06dPH3Jzc8nJySEpKYkXX3zRdSd/WeuVxRnNP3LkyBL5Y7m5ucTHx+NwOHA4HK7C9dZbbxEZGYnD4eAvf/kLQIljByua37nvmJgY+vXrR3i4NTw9YMAAunbtSkRERImAz48//pguXbrgcDi46aabOHPmDG3btsX5R/aZM2e49tprqcwf3Z7izeKSArQVkTYiEgQMBla5ryAijUXE2YbxwAJ7eQsRqWM/DgV6A7vwsHOFA55RJTE7m+CAAF64+mrIzoY334SRI6FU+qtheFytWtC06R+RQiLQrNnZvZkL0LBhQ3r06MFHH30EWL2WhIQERIQpU6aQmppKRkYGX3zxBRkZGeXuZ8uWLSxZsoS0tDTWrl1LSkqK67WBAweSkpJCeno6HTt2ZP78+fz5z3+mX79+zJgxg7S0NK655hrX+idOnGDYsGEsXbqU7du3U1xc7MryAmjcuDFbt25l5MiR57305tSlSxdXIXJG82/bto3Bgwczffp0WrduTWJiIo899hhpaWnExMSUuV5ZnNH88fHxrFmzxpUf5ozmT09PZ+vWrURERLii+T/77DPS09OZPXv2edu+detWZs+eTXZ2NmBF82/ZsoXU1FTmzJnD4cOHycvL469//Svvvfce6enpvPvuuyWi+YFqEc3vtftcVLVYRB4CPgECgAWqukNEngVSVXUVEAv8XUQU2Ag8aG/eEZhlLxdgpqpu93QbnVlNE9ZPYO/RvbQKbsWUm6YwpPMQ5h88yFe//cbC9u1pHBQEzzxj/YcfP97TzTD80UsVmNTr4EHr8uuJE1C7NmzZYhWYi+C8NNa/f3+WLFnC/PnzAVi2bBmvvfYaxcXFHDx4kKysLCIjI8vcx6ZNm4iPj6duXesPs379+rley8zMZOLEiRw5coSCggJuvvnmc7Zn165dtGnThnbt2gEwdOhQ5s6dy2i7FzZwoDUk27VrV95///0KHaOJ5q8e0fxevYlSVdcCa0ste9rt8XJgeRnbfQqU/ZPtYUM6DzkrEDDv5Eme3LOH64ODGdqsGWzfDkuWwLhx8Kc/XYpmGQaEhcH998Orr1rfL7KwAPTv35/HHnuMrVu3UlhYSNeuXfnxxx+ZOXMmKSkphIaGMmzYsHNGwJ/LsGHDWLFiBQ6Hg0WLFvH5559fVHudsf0XEtlfOpp/zJgx9OvXzzU7Zlkqsp57ND9YoZd16tTh9tsrNrGgU2Wi+evWrUtsbOwFRfM7ezFVpaoH9KulJ/bs4djp0yS1a4eIwNNPwxVXwBNPVHXTDH/z1FNWxNBTT3lkd/Xr1ycuLo7hw4e7BvJ/++036tWrR3BwMLm5ua7LZuW5/vrrWbFiBcePH+fYsWN86BZ/dOzYMcLCwjh16lSJX26lo/Cd2rdvT05ODrt37wbg7bff5oYbbqj08Zlo/uoTzW+KSymf5+fzZm4uT7RsScd69SAlBVasgLFjITS0qptn+JuwMPjiC4/0Wpzuvvtu0tPTXcXF4XAQHR1Nhw4duOeee+jVq9c5t+/SpQt33XUXDoeDW2+9le7du7tee+655+jZsye9evWiQ4cOruWDBw9mxowZREdHs2fPHtfy2rVrs3DhQu688046d+5MjRo1SEy8sEAOE81fPaP5TXClm6IzZ4hKTaXozBkyu3enbkCANQFYaqo102SDBh5qreGPTHCl4W3ni+Y3wZVVZOa+fewsLOSjzp2twrJpE3zyCcyYYQqLYRjV2rRp05g3b16Vj7U4mctitj3Hj/P8Tz9xZ5Mm3NKoEajChAnWZYlRo6q6eYZhGOc0btw4fvrpJ3r37l3VTQH8vLhM37uXDfn51ox42dkEinBnkyZM37sXPv3U6rlMmAB1655/Z4ZhGIaLXxeX7g0akJCVxaScHD7Jz2do06aM+v57utevb92Ff9VV8MADVd1MwzCMy45fj7nEhYayoH17+mdmEhYYyJK8PJaFhxO3caP1KbH5860bJw3DMIwL4tc9F7B6L23r1OHgqVOMbN6cuOBg656Ctm3hvvuqunmGYRiXJb8vLt8VFvKf4mKeuuoq5h04wIYVK6w78idPhpp+3bEzqpBzPNDdhvx8azywkg4fPkxUVBRRUVE0a9aMK6+80vXc/S7x8pROEk5KSnJFnlys2NhY2rdvT2RkJB06dOChhx4qM96/tKlTp1bq/Q4dOkRgYCBJSUmV2t44P78uLhvy80nIymJZeDjPtmnDsvbtSQgMZEN8PLjdmGQYl5pzPNBZYJw/q90v4iPxjRo1Ii0tjbS0tBLBjWlpaQRVIBCzdHFJTEzkPg/27hcvXkxGRgYZGRnUqlWL/v1Lzy14tsoWl3fffZfrrruuRLKxN/hCdH5l+fWf5inHjlljLPad93GrV7Ns9mxSpk4lroZf113Dy0Z//z1pBQXnXKd5UBA3Z2QQFhTEwZMn6Vi3LpNzcpick1Pm+lH16/NS27YX1I4tW7YwZswYCgoKaNy4MYsWLSIsLIw5c+aQlJREzZo1CQ8PZ9q0aSQlJREQEMC//vUv/vnPf7J+/Xrq16/P448/TmxsLD179mTDhg0cOXKE+fPnExMTQ2FhIcOGDSMzM5P27dtz4MAB5s6dS7du5d+zFxQUxPTp07n22mtJT0/H4XAwYMAA9u3bx4kTJ3j00UcZMWIE48aN4/jx40RFRREREcHixYvLXK8sycnJzJo1i3vuuYf9+/fTwk46f+utt5g5cyYiQmRkJG+//Ta5ubkkJibyww8/ADBv3jyaN2/O7bffTmZmJgAzZ86koKCAZ555htjYWKKiovjyyy+5++67adeuHc8//zwnT56kUaNGLF68mKZNm1JQUMDDDz9MamoqIsKkSZM4evQoGRkZvGQHm77++utkZWXx4osvXtB5rQ78urg82cptPpeTJ2HyZOIaNybulluqrlGGYQutWZOwoCD2FhXRqlYtQj18mVZVefjhh1m5ciVNmjRh6dKlTJgwgQULFjBt2jR+/PFHatWqxZEjRwgJCSExMdFVTICzJuQqLi7m22+/Ze3atUyePJl169bxyiuvEBoaSlZWFpmZmURFRVWobQEBATgcDnbu3InD4WDBggU0bNiQ48eP0717dwYNGsS0adN4+eWXSUtLc21X1nqNGjUqse99+/Zx8OBBevToQUJCAkuXLmXs2LGuiPzNmzfTuHFjV2aXM07/gw8+4PTp0xQUFJBf6pJlaSdPnnTN4Jmfn8+///1vRIQ33niD6dOnM2vWLJ577jmCg4PZvn27a73AwECmTJnCjBkzCAwMZOHChbz66qsV+jerbvy6uJQwfz7k5EBS0h9zaBiGl1Skh+G8FOYcD5zUurWrl+0JRUVFZGZm0rdvX8AKXAwLCwMgMjKSIUOGMGDAAAYMGFCh/bnH4+fYvasvv/ySRx99FIBOnTqVG+NfFvdoqrKi50sXjYqut3TpUhISEgAr82z48OGMHTuWzz77rMyI/LLi9M9XXNzzvsqL81+3bl2JmUFD7XN74403snr1ajp27MipU6dcKcyXG/8uLtHR4PZXD2BliUVFWTP+GUYVcR8PjAsNJS4kpMRzT1BVIiIi+Prrr896bc2aNWzcuJEPP/yQKVOmuP66PpfKxOOX5/Tp02zfvp2OHTtWOHq+ouslJyfzyy+/uGJSDhw4wPfff39B7XOPzQfOeh/3MMqKxv47PfDAA0ydOpUOHTpUiwDKyvLvgQUvzFNuGJ5w1nhgaCjLwsNJKSO2vrJq1apFXl6eq7icOnWKHTt2cObMGfbt20dcXBwvvPACR48epaCgoNzY/HPp1asXy5YtAyArK6tCRerUqVOMHz+eli1bEhkZWW70PEBgYKArsv5c6zllZ2dTUFDAzz//7IrOHz9+PMnJyeVG5JcVp9+0aVN+/fVXDh8+TFFREatXry73eMqL8+/bty9z5851PXf2hnr27Mm+fft45513XMnVlyP/Li5PPQWlB+4DAjw2d4ZhVNaTrVqd1UOJCw0tOU54kWrUqMHy5cv529/+hsPhICoqis2bN3P69GnuvfdeOnfuTHR0NI888gghISFnxdRXxKhRo8jLyyM8PJyJEycSERFBcHBwmesOGTKEyMhIOnXqxO+//87KlSuB8qPnAUaMGOG6hHeu9ZySk5OJj48vsWzQoEEkJyeXG5FfVpx+YGAgTz/9ND169KBv374lphcorbw4/4kTJ5Kfn0+nTp1wOBxs2LDB9VpCQgK9evVyXSq7LKmq176AW4BdwG5gXBmvXwWsBzKAz4EW9vIo4Gtgh/3aXed7r65du2ql3Hefao0aqqAaFKQ6alTl9mMY55GVlVXVTbjkiouL9fjx46qqunv3bm3durUWFRVVcauqv9tuu03XrVvn8f2W9TOINe28x3//e63nIiIBwFzgViAcuFtEwkutNhN4S1UjgWeBv9vLC4H7VDUCq0C9JCIhXmnotGl/XBozvRbD8KjCwkJ69+6Nw+EgPj6eV155pUL31PirI0eO0K5dO+rUqeOa8Oxy5c0B/R7AblX9AUBElgD9gSy3dcKBMfbjDcAKAFXNdq6gqgdE5FegCXD+W3YvlBfmKTcMw9KgQQMudhI/fxISEkJ2dvb5V7wMeHPM5Upgn9vz/fYyd+nAQPtxPNBAREp8blBEegBBwJ5S2yIiI0QkVURS8/LyKt9SD89TbhjlUR+Z+dW4/Fzqn72qHtB/HLhBRLYBNwA/A6edL4pIGPA2cL+qnim9saq+pqrdVLVbkyZNKt8KL8xTbhil1a5dm8OHD5sCY1xyqsrhw4epXbv2JXtPb14W+xlo6fa8hb3MRVUPYPdcRKQ+MEhVj9jPrwDWABNU9ezPFBrGZaZFixbs37+fi+plG0Yl1a5d2xVzcyl4s7ikAG1FpA1WURkM3OO+gog0Bv5j90rGAwvs5UHAB1iD/cu92EbDuGQCAwNdd2cbhq/z2mUxVS0GHgI+Ab4DlqnqDhF5VkT62avFArtEJBtoCkyxlycA1wPDRCTN/qpYKJFhGIZR5cRXrv9269ZNzadSDMMwLoyIbFHV8mOqK6mqB/QNwzAMH+QzPRcRyQN+usDNGgOHvNCc6swfjxn887j98ZjBP4/7Yo75KlW9iI/bls1niktliEiqN7qD1Zk/HjP453H74zGDfx53dTxmc1nMMAzD8DhTXAzDMAyP8/fi8lpVN6AK+OMxg38etz8eM/jncVe7Y/brMRfDMAzDO/y952IYhmF4gSkuhmEYhsf5ZXERkVtEZJeI7BaRcVXdHm8RkZYiskFEskRkh4g8ai9vKCKfisj39vfLeC7VsolIgIhsE5HV9vM2IvKNfc6X2vl1PkNEQkRkuYjsFJHvROS//OQ8P2b/bGeKSLKI1PbFcy0iC0TkVxHJdFtW5vkVyxz7+DNEpEtVtNnviksFZ8j0FcXAWFUNB64DHrSPdRywXlXbYk0z7YsF9lGsTDunF4AXVfVaIB/4nypplffMBj5W1Q6AA+vYffo8i8iVwCNAN1XtBARgBeT64rlehDUrr7vyzu+tQFv7awQw7xK1sQS/Ky64zZCpqicB5wyZPkdVD6rqVvvxMaxfOFdiHe+b9mpvAgOqpoXeISItgNuAN+znAtwIOBO2feqYRSQYK+h1PoCqnrSnrvDp82yrCdQRkZpAXeAgPniuVXUj8J9Si8s7v/2xEuXVnq4kxJ4b65Lyx+JSkRkyfY6ItAaigW+Apqp60H7pF6xEal/yEvAk4JxgrhFwxE7qBt87522APGChfSnwDRGph4+fZ1X9GZgJ7MUqKkeBLfj2uXZX3vmtFr/j/LG4+B17Irb3gNGq+pv7a2p9Ft1nPo8uIrcDv6rqlqpuyyVUE+gCzFPVaOB3Sl0C87XzDGCPMfTHKq7NgXqcfenIL1TH8+uPxeW8M2T6EhEJxCosi1X1fXtxrrObbH//tara5wW9gH4ikoN1yfNGrPGIEPvSCfjeOd8P7FfVb+zny7GKjS+fZ4A+wI+qmqeqp4D3sc6/L59rd+Wd32rxO84fi4trhkz7UySDgVVV3CavsMca5gPfqeo/3F5aBQy1Hw8FVl7qtnmLqo5X1Raq2hrr3H6mqkOADcAd9mq+dsy/APtEpL296CYgCx8+z7a9wHUiUtf+WXcet8+e61LKO7+rgPvsT41dBxx1u3x2yfjlHfoi8t9Y1+UDgAWqOuU8m1yWRKQ3sAnYzh/jD/+LNe6yDGiFNU1BgqqWHiy87IlILPC4qt4uIldj9WQaAtuAe1W1qCrb50n2TK1vAEHAD8D9WH88+vR5FpHJwF1Yn4zcBjyANb7gU+daRJKxZu5tDOQCk4AVlHF+7UL7MtYlwkLgflW95DMp+mVxMQzDMLzLHy+LGYZhGF5miothGIbhcaa4GIZhGB5niothGIbhcaa4GIZhGB5niovhl+y06JtLLRstIuWG/InI5yLSzfutO+t9H7GTjheX8VoPEdlop3w7o1/qXuo2GkZpNc+/imH4pGSsmyw/cVs2GCuTrLoZBfRR1f3uC0WkKfAuMFhVv7aX3QE0wLq/wTCqjOm5GP5qOXCbc64PO9izObBJROaJSKo9T8jksjYWkQK3x3eIyCL7cRMReU9EUuyvXvbyG0Qkzf7aJiINytjnGHtekkwRGW0vSwKuBj4SkcdKbfIg8KazsACo6nJVza3sP4pheIrpuRh+yb6T+VusuS9WYvValqmqisgE+/UAYL2IRKpqRgV3PRtrLpEvRaQVVs+oI/A48KCqfmUHiZ5w30hEumLdVd8TEOAbEflCVRNF5BYgTlUPlXqvTvwRuW4Y1YrpuRj+zHlpDPt7sv04QUS2YkWHRGBNKldRfYCXRSQNK+PpCruYfAX8Q0QeAULcIuGdegMfqOrvqlqAFcIYU5mDMozqwBQXw5+tBG6yp4Gtq6pbRKQNVi/jJlWNBNYAtcvY1j03yf31GsB1qhplf12pqgWqOg0r96oO8JWIdPBA+3cAXT2wH8PwOFNcDL9l9xA2AAv4o9dyBdZ8KEftAfNby9k8V0Q6ikgNIN5t+f8BDzuf2IGSiMg1qrpdVV/ASuYuXVw2AQPshN969j43necQXgaGikhPt/cbaLfbMKqUKS6Gv0vGmnM+GUBV07Euh+0E3sG6nFWWccBqYDPWLIhOjwDdRCRDRLKARHv5aHugPgM4BXzkvjN7OupFwLdYqdVvqOq2czXcHrgfDMy0P4r8HXAzcKwCx20YXmVSkQ3DMAyPMz0XwzAMw+NMcTEMwzA8zhQXwzAMw+NMcTEMwzA8zhQXwzAMw+NMcTEMwzA8zhQXwzAMw+P+H0sZ0g8TxRgUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5323XiSdhkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726dbee5-24a4-4b7e-9a88-922d81c2899c"
      },
      "source": [
        "print('choose the best choice of parameters, and train with the whole training dataset and report the accuracy.')\n",
        "rbf_default_c_optimal = svm.SVC(C = 30.0, kernel = 'rbf', gamma = 'auto')\n",
        "rbf_default_c_optimal.fit(train_data, train_label.flatten())\n",
        "\n",
        "print('training data accuracy: ' + str(100 * rbf_default_c_optimal.score(train_data, train_label)) + '%')\n",
        "print('validation data accuracy: ' + str(100 * rbf_default_c_optimal.score(validation_data, validation_label)) + '%')\n",
        "print('testing data accuracy: ' + str(100 * rbf_default_c_optimal.score(test_data, test_label)) + '%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "choose the best choice of parameters, and train with the whole training dataset and report the accuracy.\n",
            "training data accuracy: 98.372%\n",
            "validation data accuracy: 97.1%\n",
            "testing data accuracy: 97.04%\n"
          ]
        }
      ]
    }
  ]
}